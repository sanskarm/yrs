# -*- coding: utf-8 -*-
"""yrs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11LhULKFMlCDYfN_8l7dz3AnrpglmBr5h
"""

from google.colab import drive
drive.mount('/content/drive')

dataset = []

import re
from textblob import TextBlob
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords

with open('/content/drive/MyDrive/yrs/asan.txt', 'r') as f:
  with open('/content/drive/MyDrive/yrs/cleanup.txt', 'w') as clean:
    for line in f:
      line = line.lower()
      line = line.replace(",","")
      line = line.replace("\n"," ")
      line = line.replace(".","\n")
      line = line.replace("?","")
      line = line.replace("<","")
      line = line.replace(">","")
      line = line.replace("/","")
      line = line.replace(";","")
      line = line.replace(":","")
      line = line.replace("'","")
      line = line.replace("-"," ")
      line = line.replace("+","")
      line = line.replace("=","")
      line = line.replace("!","")
      line = line.replace("&","")
      line = line.replace("`","")
      line = line.replace("~","")
      line = line.replace("@","")
      line = line.replace("#","")
      line = line.replace("$","")
      line = line.replace("%","")
      line = line.replace("^","")
      line = line.replace("_","")
      line = line.replace("|","")
      line = line.replace("  "," ")
      line = re.sub(r'[0-9]+', ' ', line)
      line = re.sub(r'[^A-Za-z0-9\n]+', ' ', line)
      clean.write(line)

with open('/content/drive/MyDrive/yrs/cleanup.txt', 'r') as clean:
  for line in clean:
    dataset.append(line)

len(dataset)

dataset[0]

from keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.utils import np_utils
import numpy as np

oov_token = "<OOV>"
embed_size = 100
window_size = 3

tokenizer = Tokenizer(oov_token=oov_token)
tokenizer.fit_on_texts(dataset)
word_index = tokenizer.word_index
sequence = tokenizer.texts_to_sequences(dataset)
word_index['PAD']=0

print(len(word_index))

vocab_size = len(word_index)

index_to_word_map = {}
for word,index in word_index.items():
  index_to_word_map[index] = word

print('Vocabulary Sample:', list(word_index.items())[:10])

print(index_to_word_map[5800])

def target_and_context_generator(data, window_size, vocab_size):
  A = []
  B = []
  for line in data:
    sentence_length = len(line)
    for index,word in enumerate(line):
      target=[]
      context=[]
      start = index - window_size
      end = index + window_size + 1
      context.append([line[i] 
                                 for i in range(start, end) 
                                 if 0 <= i < sentence_length 
                                 and i != index])
      target.append(word)
      context_len = 2*window_size
      x = pad_sequences(context,maxlen=context_len)
      y = np_utils.to_categorical(target,vocab_size)
      A.append(x)
      B.append(y)
  return A,B

A,B = target_and_context_generator(data=sequence, window_size=window_size, vocab_size=vocab_size)

len(A)

A = np.array(A)

B = np.array(B)

np.shape(A)

A

import keras.backend as K
from keras.models import Sequential
from keras.layers import Dense, Embedding, Lambda
import tensorflow as tf

model = Sequential()
model.add(Embedding(input_dim=vocab_size,output_dim=embed_size,input_length=2*window_size))
model.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))
model.add(Dense(vocab_size, activation='softmax'))
model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), optimizer='adam',metrics=['accuracy'])

print(model.summary())

num_epochs=5
model.fit(A,B,batch_size=20000,epochs=num_epochs)



import io

out_v = io.open('vecs.tsv', 'w', encoding='utf-8')
out_m = io.open('meta.tsv', 'w', encoding='utf-8')
for word_num in range(1, vocab_size):
  word = index_to_word_map[word_num]
  embeddings = weights[word_num]
  out_m.write(word + "\n")
  out_v.write('\t'.join([str(x) for x in embeddings]) + "\n")
out_v.close()
out_m.close()

try:
  from google.colab import files
except ImportError:
  pass
else:
  files.download('vecs.tsv')
  files.download('meta.tsv')